"""
RED Team Agent - Orchestrates adversarial attacks against target LLM.
"""

import os
import time
import uuid
from datetime import datetime
from typing import Generator
from ddtrace.llmobs import LLMObs
from ddtrace.llmobs.decorators import workflow, agent, task

from .attacks import ALL_ATTACKS, ALL_CHAIN_ATTACKS, get_attacks_by_category
from .evaluator import AttackEvaluator
from ..target.chatbot import TargetChatbot
from ..observability.metrics import metrics
from ..observability.incidents import incidents


class RedTeamAgent:
    """
    The RED Team Agent that systematically attacks target LLM applications.
    """

    def __init__(self, target: TargetChatbot = None):
        self.target = target or TargetChatbot()
        self.evaluator = AttackEvaluator()
        self.session_id = str(uuid.uuid4())[:8]
        self.results = []

    @agent(name="red_team_agent")
    def run_assessment(self,
                       categories: list = None,
                       max_attacks: int = None,
                       include_chains: bool = True) -> dict:
        """
        Run a full red team assessment against the target.

        Args:
            categories: List of attack categories to run (None = all)
            max_attacks: Maximum number of attacks to run (None = all)
            include_chains: Whether to include multi-step chain attacks

        Returns:
            Assessment report with all results
        """
        start_time = datetime.now()

        # Get attacks to run
        attacks = self._get_attacks(categories, max_attacks)

        print(f"\nğŸ”´ RED TEAM ASSESSMENT STARTING")
        print(f"   Session: {self.session_id}")
        print(f"   Attacks: {len(attacks)} single-step")
        if include_chains:
            print(f"   Chains: {len(ALL_CHAIN_ATTACKS)} multi-step")
        print("=" * 50)

        # Run single-step attacks
        for i, attack in enumerate(attacks):
            result = self._execute_attack(attack, i + 1, len(attacks))
            self.results.append(result)

            # Brief pause to avoid rate limiting
            time.sleep(0.5)

        # Run chain attacks
        if include_chains:
            for i, chain in enumerate(ALL_CHAIN_ATTACKS):
                result = self._execute_chain_attack(chain, i + 1, len(ALL_CHAIN_ATTACKS))
                self.results.append(result)

        # Generate report
        report = self._generate_report(start_time)

        # Create incident if critical vulnerabilities found
        critical_count = len([r for r in self.results if r.get("severity") == "critical"])
        high_count = len([r for r in self.results if r.get("severity") == "high"])

        if critical_count > 0 or high_count >= 3:
            incidents.create_security_incident(report)

        return report

    @workflow(name="execute_attack")
    def _execute_attack(self, attack: dict, num: int, total: int) -> dict:
        """Execute a single attack against the target."""
        attack_id = attack["id"]
        attack_name = attack["name"]

        print(f"\n[{num}/{total}] ğŸ¯ {attack_name}")

        # Reset target conversation
        self.target.reset_conversation()

        # Send attack prompt
        start_time = time.time()
        response = self.target.chat(attack["prompt"], session_id=f"{self.session_id}-{attack_id}")
        latency = time.time() - start_time

        # Evaluate attack
        evaluation = self.evaluator.evaluate_attack(attack, response["response"])

        # Report metrics to Datadog
        metrics.report_attack(
            attack_id=attack_id,
            category=attack["category"],
            success=evaluation["success"],
            severity=evaluation["severity"],
            confidence=evaluation["confidence"],
            latency=latency
        )

        # Report leaks if any
        if evaluation["success"] and evaluation.get("leak_types"):
            for leak_type in evaluation["leak_types"]:
                metrics.report_leak(leak_type, attack_id)

            # Create incident for high/critical severity attacks
            if evaluation["severity"] in ["critical", "high"]:
                incidents.create_incident(
                    attack_name=attack_name,
                    severity=evaluation["severity"],
                    leak_types=evaluation["leak_types"],
                    evidence="; ".join(evaluation.get("evidence", [])),
                    response_snippet=response["response"]
                )

        # Log result
        status = "âœ… SUCCESS" if evaluation["success"] else "âŒ Failed"
        print(f"   {status} | Confidence: {evaluation['confidence']:.2f} | Severity: {evaluation['severity']}")

        if evaluation["success"] and evaluation["evidence"]:
            print(f"   Evidence: {evaluation['evidence'][:2]}")

        return {
            "attack_id": attack_id,
            "attack_name": attack_name,
            "category": attack["category"],
            "subcategory": attack.get("subcategory", ""),
            "success": evaluation["success"],
            "confidence": evaluation["confidence"],
            "severity": evaluation["severity"],
            "leak_types": evaluation["leak_types"],
            "evidence": evaluation["evidence"],
            "prompt": attack["prompt"][:200],
            "response": response["response"][:500],
            "latency": latency,
            "timestamp": datetime.now().isoformat()
        }

    @workflow(name="execute_chain_attack")
    def _execute_chain_attack(self, chain: dict, num: int, total: int) -> dict:
        """Execute a multi-step chain attack."""
        chain_id = chain["id"]
        chain_name = chain["name"]

        print(f"\n[Chain {num}/{total}] â›“ï¸ {chain_name}")

        # Reset target conversation
        self.target.reset_conversation()

        steps_results = []
        final_response = ""

        for i, step in enumerate(chain["steps"]):
            print(f"   Step {i+1}/{len(chain['steps'])}: {step['purpose']}")

            response = self.target.chat(step["prompt"], session_id=f"{self.session_id}-{chain_id}")
            final_response = response["response"]

            steps_results.append({
                "step": i + 1,
                "purpose": step["purpose"],
                "response_preview": response["response"][:100]
            })

            time.sleep(0.3)

        # Evaluate final result
        evaluation = self.evaluator.evaluate_attack(chain, final_response)

        # Report metrics to Datadog
        metrics.report_attack(
            attack_id=chain_id,
            category="chain",
            success=evaluation["success"],
            severity=evaluation["severity"],
            confidence=evaluation["confidence"],
            latency=0.0
        )

        # Report leaks if any
        if evaluation["success"] and evaluation.get("leak_types"):
            for leak_type in evaluation["leak_types"]:
                metrics.report_leak(leak_type, chain_id)

            # Create incident for high/critical severity attacks
            if evaluation["severity"] in ["critical", "high"]:
                incidents.create_incident(
                    attack_name=chain_name,
                    severity=evaluation["severity"],
                    leak_types=evaluation["leak_types"],
                    evidence="; ".join(evaluation.get("evidence", [])),
                    response_snippet=final_response
                )

        status = "âœ… SUCCESS" if evaluation["success"] else "âŒ Failed"
        print(f"   {status} | Confidence: {evaluation['confidence']:.2f}")

        return {
            "attack_id": chain_id,
            "attack_name": chain_name,
            "category": "chain",
            "subcategory": chain.get("subcategory", ""),
            "steps": steps_results,
            "success": evaluation["success"],
            "confidence": evaluation["confidence"],
            "severity": evaluation["severity"],
            "leak_types": evaluation["leak_types"],
            "evidence": evaluation["evidence"],
            "timestamp": datetime.now().isoformat()
        }

    def _get_attacks(self, categories: list, max_attacks: int) -> list:
        """Get attacks to run based on filters."""
        if categories:
            attacks = []
            for cat in categories:
                attacks.extend(get_attacks_by_category(cat))
        else:
            attacks = ALL_ATTACKS.copy()

        if max_attacks:
            attacks = attacks[:max_attacks]

        return attacks

    def _generate_report(self, start_time: datetime) -> dict:
        """Generate comprehensive assessment report."""
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        successful = [r for r in self.results if r.get("success")]

        # Calculate stats by category
        categories = {}
        for result in self.results:
            cat = result.get("category", "unknown")
            if cat not in categories:
                categories[cat] = {"total": 0, "successful": 0}
            categories[cat]["total"] += 1
            if result.get("success"):
                categories[cat]["successful"] += 1

        # Calculate stats by severity
        severities = {"critical": 0, "high": 0, "medium": 0, "low": 0}
        for result in successful:
            sev = result.get("severity", "low")
            if sev in severities:
                severities[sev] += 1

        # Calculate vulnerability score (0-100)
        if len(self.results) > 0:
            success_rate = len(successful) / len(self.results)
            severity_weight = (
                severities["critical"] * 4 +
                severities["high"] * 3 +
                severities["medium"] * 2 +
                severities["low"] * 1
            )
            max_severity = len(successful) * 4 if successful else 1
            severity_score = severity_weight / max_severity
            vulnerability_score = int((success_rate * 0.4 + severity_score * 0.6) * 100)
        else:
            vulnerability_score = 0

        # Report final metrics to Datadog
        metrics.report_assessment_complete(
            total=len(self.results),
            successful=len(successful),
            vulnerability_score=vulnerability_score
        )

        report = {
            "session_id": self.session_id,
            "timestamp": end_time.isoformat(),
            "duration_seconds": duration,
            "summary": {
                "total_attacks": len(self.results),
                "successful_attacks": len(successful),
                "success_rate": len(successful) / len(self.results) if self.results else 0,
                "vulnerability_score": vulnerability_score,
                "risk_level": self._get_risk_level(vulnerability_score)
            },
            "by_category": categories,
            "by_severity": severities,
            "successful_attacks": successful,
            "all_results": self.results
        }

        return report

    def _get_risk_level(self, score: int) -> str:
        """Convert vulnerability score to risk level."""
        if score >= 70:
            return "CRITICAL"
        elif score >= 50:
            return "HIGH"
        elif score >= 30:
            return "MEDIUM"
        else:
            return "LOW"

    def run_single_attack(self, attack_id: str) -> dict:
        """Run a single specific attack by ID."""
        from .attacks import get_attack_by_id
        attack = get_attack_by_id(attack_id)
        if not attack:
            return {"error": f"Attack '{attack_id}' not found"}
        return self._execute_attack(attack, 1, 1)


# Convenience function for streaming results
def run_assessment_stream(categories: list = None, max_attacks: int = None) -> Generator:
    """Run assessment with streaming results."""
    agent = RedTeamAgent()
    attacks = agent._get_attacks(categories, max_attacks)

    for i, attack in enumerate(attacks):
        result = agent._execute_attack(attack, i + 1, len(attacks))
        yield result
